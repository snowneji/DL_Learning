{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "#### Author: Yifan Wang\n",
    "#### Theory Support:  Dr. Huang. Shangwen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful Resources:\n",
    "    \n",
    "     https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "    \n",
    "     https://www.quora.com/Why-does-GBM-use-regression-on-pseudo-residuals\n",
    "     \n",
    "     http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ideas of Gradient Boosting:\n",
    "\n",
    "    1. fit a weak learner\n",
    "    2. identify and train a model on the pseudo residual and the gradient\n",
    "    3. update the weak learner using the gradient of pseudo residual \n",
    "    (repeat step 2 and 3 until converge)\n",
    "    \n",
    "    \n",
    "\n",
    "Sounds easy right? but it can be tricky to implement that:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Learner (will be used to learn the residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CART_regressor():\n",
    "    'Implementation of CART(Classification and Regression Tree) Decision Tree in Python, majorly in NumPy'\n",
    "    def __init__(self,least_children_num,verbose=True):\n",
    "        self.least_children_num = least_children_num\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    \n",
    "    def fit(self,tmp_x,tmp_y):\n",
    "        def fit_tree(tmp_x,tmp_y):\n",
    "        #     Exit Condition 0:\n",
    "            # Exit Condition 1:\n",
    "            if \\\n",
    "            len(tmp_y) < self.least_children_num or len(np.unique(tmp_y))==1:\n",
    "\n",
    "                if self.verbose:\n",
    "                    print('exit condition:')\n",
    "                    print('tmp_y:')\n",
    "                    print(tmp_y)\n",
    "\n",
    "                mode_val = self.mean(tmp_y.flatten().tolist())\n",
    "                return([np.nan, mode_val, np.nan, np.nan]) # Leaf Node: format [feat,splitval,]\n",
    "\n",
    "            # Otherwise Split:\n",
    "            if self.verbose:\n",
    "                print(\"start....subset Y len {}\".format(len(tmp_y)))\n",
    "\n",
    "\n",
    "            split_row,split_col = self.decide_split_data(tmp_x,tmp_y)\n",
    "\n",
    "            if not split_row and not split_col:\n",
    "#                 print('no better split...return mean')\n",
    "                mode_val = self.mean(tmp_y.flatten().tolist())\n",
    "                return([np.nan, mode_val, np.nan, np.nan])\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"split on:\")\n",
    "                print(split_row,split_col)\n",
    "\n",
    "\n",
    "            split_vec = tmp_x[:,split_col]\n",
    "            split_val = tmp_x[split_row,split_col]\n",
    "\n",
    "            # Recursively Split to left and right branches:\n",
    "            left_ind = np.where(split_vec<split_val)[0].tolist()\n",
    "            right_ind = np.where(split_vec>=split_val)[0].tolist()\n",
    "            left_dat,left_y = tmp_x[left_ind,:],tmp_y[left_ind,]\n",
    "            right_dat,right_y = tmp_x[right_ind,:],tmp_y[right_ind,]\n",
    "\n",
    "            left_tree = fit_tree(left_dat,left_y)\n",
    "            right_tree = fit_tree(right_dat,right_y)\n",
    "\n",
    "            if isinstance(left_tree, list): # If list, tree len 1\n",
    "                len_l_tree = 1\n",
    "            else:\n",
    "                len_l_tree = left_tree.shape[0] # If array, tree len >1\n",
    "\n",
    "            root = [split_col,split_val,1,len_l_tree+1] # Format [split_col, split_val, left_tree_relative_idx, right_tree_relative_idx]\n",
    "            return(np.vstack([root,left_tree,right_tree]))\n",
    "        \n",
    "        tree = fit_tree(tmp_x,tmp_y)\n",
    "        self.tree = tree\n",
    "\n",
    "    \n",
    "\n",
    "    def decide_split_data(self,x,y):\n",
    "        'Given subset of X,Y, search for the best splitting node based on: MSE reduction'\n",
    "        def _MSE(tmp_y):\n",
    "            'Key Metrics of building a decision tree. Specifically MSE'\n",
    "            output = 0\n",
    "            mean_val = np.mean(tmp_y)\n",
    "            \n",
    "            for i in range(len(tmp_y)):\n",
    "                \n",
    "                tmp  = (tmp_y[i] - mean_val)**2\n",
    "                output += tmp\n",
    "            output /= len(tmp_y)\n",
    "            return output\n",
    "\n",
    "        \n",
    "        #---\n",
    "        m,n = x.shape\n",
    "        best_red = 0\n",
    "        split_row, split_col = None,None\n",
    "\n",
    "        previous_mse = _MSE(y)\n",
    "        for col in range(n):\n",
    "            tmp_vec = x[:,col].ravel()\n",
    "\n",
    "            for row in range(m):\n",
    "                val = tmp_vec[row]\n",
    "                # >= & < is my convention here:\n",
    "                if val!=np.max(tmp_vec) and val!= np.min(tmp_vec):\n",
    "                    left_b = np.where(tmp_vec<val)[0].tolist()\n",
    "                    right_b = np.where(tmp_vec>=val)[0].tolist()\n",
    "\n",
    "                    new_mse = \\\n",
    "                    (len(y[left_b])/len(y))*_MSE(y[left_b]) + \\\n",
    "                    (len(y[right_b])/len(y))*_MSE(y[right_b])\n",
    "\n",
    "\n",
    "    #                 print('new entropy: %f'%new_ent)\n",
    "                    mse_red = previous_mse - new_mse\n",
    "\n",
    "                    if mse_red > best_red:\n",
    "                        split_row, split_col = row,col\n",
    "                        best_red = mse_red\n",
    "#                         if self.verbose:\n",
    "#                             print('better red:{}'.format(mse_red))\n",
    "#                             print()\n",
    "\n",
    "        return split_row, split_col\n",
    "                \n",
    "                \n",
    "\n",
    "    def mean(self, x_list):\n",
    "        'calculate the mean'\n",
    "        return np.mean(x_list)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, tmp_test_array):\n",
    "        'Wrap-up fun for prediction'\n",
    "        def query(tree,tmp_test_array):\n",
    "            'Test for single example'\n",
    "            assert len(tmp_test_array.shape) == 2, \"Make sure your test data is 2d array\"\n",
    "        #     print(tree)\n",
    "            if isinstance(tree,list):\n",
    "                start_node = tree # only the 1 row in data\n",
    "            else:\n",
    "                start_node = tree[0,:] # Iteratively hit first row\n",
    "                \n",
    "    \n",
    "            test_feat,test_val,left_tree_jump,right_tree_jump = start_node[0],start_node[1],start_node[2],start_node[3]\n",
    "\n",
    "            # Exit Condition:\n",
    "            if np.isnan(test_feat) and np.isnan(left_tree_jump) and np.isnan(right_tree_jump):\n",
    "                pred = test_val;\n",
    "                return pred \n",
    "            #Test:\n",
    "            if tmp_test_array[0,int(test_feat)] < test_val:\n",
    "                # If <, go left branch:\n",
    "                jump_loc = left_tree_jump\n",
    "                pred = query(tree[int(jump_loc):,],tmp_test_array)\n",
    "\n",
    "            else:\n",
    "                # If >=, go right branch:\n",
    "                jump_loc = right_tree_jump\n",
    "                pred = query(tree[int(jump_loc):,],tmp_test_array)\n",
    "\n",
    "            return pred\n",
    "\n",
    "        assert len(tmp_test_array.shape) == 2, \"Make sure your test data is 2d array\"\n",
    "        result = []\n",
    "\n",
    "        for i in range(tmp_test_array.shape[0]):\n",
    "            inp = tmp_test_array[i,:].reshape(1,-1)\n",
    "            result.append(query(self.tree,inp))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data and CV Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "idx = [i for i in range(len(y))]\n",
    "np.random.seed(1028)\n",
    "np.random.shuffle(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be using 113 validation examples\n"
     ]
    }
   ],
   "source": [
    "val_ratio = 0.2\n",
    "val_num = int(len(y)*val_ratio)\n",
    "\n",
    "print(\"We will be using {} validation examples\".format(val_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid = X[val_num:], X[:val_num]\n",
    "y_train,y_valid = y[val_num:], y[:val_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classification Learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_Boosting_Classifier():\n",
    "    def __init__(self,LEAST_CHILDREN_NUM,LEARNING_RATE,N_TREE,VERBOSE=False):\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.LEAST_CHILDREN_NUM = LEAST_CHILDREN_NUM\n",
    "        self.N_TREE = N_TREE\n",
    "        self.trees = {}\n",
    "        self.VERBOSE = VERBOSE\n",
    "        \n",
    "    def ohe(self,x):\n",
    "        \"\"\"\n",
    "        One-Hot-Encoding a 1d Matrix\n",
    "        \"\"\"\n",
    "        m = len(x)\n",
    "        n = len(np.unique(x))\n",
    "        output = np.zeros((m,n))\n",
    "        for i in range(len(x)):\n",
    "            output[i,int(x[i])] = 1\n",
    "        return output\n",
    "        \n",
    "    def KL_divergence(self,arr_x,arr_y):\n",
    "        \"\"\"\n",
    "        Calculate KL Divergence as our loss function\n",
    "        \"\"\"\n",
    "        output = arr_x * np.log(arr_x / (arr_y+1)+1)\n",
    "        return np.nansum(output, axis=0)\n",
    "\n",
    "    def find_residual(self,y,pred):\n",
    "        \"\"\"\n",
    "        find d_residual\n",
    "        \"\"\"\n",
    "        n_class = pred.shape[1]\n",
    "        d_loss = np.zeros(y.shape)\n",
    "        for class_num in range(n_class):\n",
    "            d_loss[:,class_num] = y[:,class_num] - pred[:,class_num]  # negative gradient\n",
    "        return d_loss\n",
    "\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        sequentially train the boosting model\n",
    "        \"\"\"\n",
    "\n",
    "        original_y = y.copy()\n",
    "        # first ohe y:\n",
    "        y = self.ohe(y)\n",
    "        self.n_class = y.shape[1]\n",
    "        # initialize:\n",
    "        y_train_pred = np.zeros((len(y),self.n_class)) + (1.0/self.n_class)\n",
    "        self.trees['initial_model'] = None # leave an interface if initial learner is a model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for round_i in range(self.N_TREE):\n",
    "            \n",
    "            residual = self.find_residual(y_train_pred,y) # find residual\n",
    "            print(\"Round-%d started\"%round_i)\n",
    "#             kl_div = self.KL_divergence(original_y,y_train_pred.argmax(axis=1))\n",
    "#             print(\"KL Divergence {}\".format(kl_div))\n",
    "#             print()\n",
    "            models = {}\n",
    "            # fit on residual and update each class\n",
    "            for class_i in range(self.n_class):\n",
    "                model = CART_regressor(self.LEAST_CHILDREN_NUM,False)\n",
    "                model.fit(X,residual[:,class_i])  \n",
    "                models['model%d'%class_i] = model\n",
    "\n",
    "                update_val = self.LEARNING_RATE*residual[:,class_i]\n",
    "                y_train_pred[:,class_i] = y_train_pred[:,class_i] - update_val\n",
    "\n",
    "            self.trees['tree%d'%round_i] = models # Save the trees for prediction\n",
    "        \n",
    "    def evaluate(self,X,y):\n",
    "        \"\"\"\n",
    "        Evaluation,\n",
    "        basically prediction, but requires label to evaluate\n",
    "        \"\"\"\n",
    "\n",
    "        y_valid_pred = np.zeros((len(X),self.n_class)) + (1.0/self.n_class)\n",
    "    \n",
    "        for round_i in range(len(self.trees)-1): # exclude initial model\n",
    "\n",
    "            models = self.trees['tree%d'%round_i]\n",
    "            for class_i in range(self.n_class):\n",
    "#                 print(\"round %d\"%round_i)\n",
    "#                 print('tree-%d'%round_i)\n",
    "#                 print(\"class %d\"%class_i)\n",
    "                \n",
    "                model = models['model%d'%class_i]\n",
    "                temp_pred = model.predict(X)\n",
    "#                 print(temp_pred)\n",
    "                y_valid_pred[:,class_i] =  y_valid_pred[:,class_i] - self.LEARNING_RATE*np.array(temp_pred)\n",
    "                \n",
    "                del model\n",
    "                del temp_pred\n",
    "        \n",
    "            \n",
    "                \n",
    "            temp_pred = y_valid_pred.argmax(axis=1)\n",
    "            accuracy = sum(temp_pred==y)/len(y)\n",
    "        print(\"Accuracy is %.4f\"%accuracy)\n",
    "\n",
    "        y_pred = y_valid_pred.argmax(axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gradient_Boosting_Classifier(\n",
    "    LEARNING_RATE = 0.01,\n",
    "    LEAST_CHILDREN_NUM = 10,\n",
    "    N_TREE = 10, # \n",
    "    VERBOSE = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-0 started\n",
      "Round-1 started\n",
      "Round-2 started\n",
      "Round-3 started\n",
      "Round-4 started\n",
      "Round-5 started\n",
      "Round-6 started\n",
      "Round-7 started\n",
      "Round-8 started\n",
      "Round-9 started\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cv prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample accuracy 88.5%, slightly better than single CART classification tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
