{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Tutorial\n",
    "\n",
    "###### using Gibbs Sampling\n",
    "\n",
    "\n",
    "## Author: Yifan Wang @ July 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me to understand LDA, I found these blog posts are particularly easy to understand:\n",
    "\n",
    "http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "\n",
    "https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "\n",
    "\n",
    "\n",
    "But still, reading the Wikipedia page and original paper is important:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf  (it's 30 pages long though >_< )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Libraries:\n",
    "import numpy as np\n",
    "\n",
    "# and yes, we will only use NumPy to build this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    'apple banana are delicious food',\n",
    "    'video game go play in game studio',\n",
    "    'lunch food is fruit apple banana icecream',\n",
    "    'warcraft or starcraft or overwatch best game',\n",
    "    'chocolate or banana or icecream the most delicious food',\n",
    "    'banana apple smoothie is  best for lunch or dinner',\n",
    "    'video game is good for geeks',\n",
    "    'what to eat for dinner banana or chocolate',\n",
    "    'which game company is better ubisoft or blizzard',\n",
    "    'play game on ps4 or xbox',\n",
    "    'banana is less sweet icecream is more sweet',\n",
    "    'chocolate icecream taste more delicious than banana'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Pre-process\n",
    "\n",
    "We will just do basic lower-case and tokenization\n",
    "'''\n",
    "stopwords = ['to','or','is','the','and','in','for','are','on','go','best','than']\n",
    "data = [doc.lower().split(' ') for doc in data]\n",
    "data = [[i for i in doc if i!=''] for doc in data]\n",
    "data = [[i for i in doc if i not in stopwords] for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'delicious', 'food'],\n",
       " ['video', 'game', 'play', 'game', 'studio'],\n",
       " ['lunch', 'food', 'fruit', 'apple', 'banana', 'icecream'],\n",
       " ['warcraft', 'starcraft', 'overwatch', 'game'],\n",
       " ['chocolate', 'banana', 'icecream', 'most', 'delicious', 'food'],\n",
       " ['banana', 'apple', 'smoothie', 'lunch', 'dinner'],\n",
       " ['video', 'game', 'good', 'geeks'],\n",
       " ['what', 'eat', 'dinner', 'banana', 'chocolate'],\n",
       " ['which', 'game', 'company', 'better', 'ubisoft', 'blizzard'],\n",
       " ['play', 'game', 'ps4', 'xbox'],\n",
       " ['banana', 'less', 'sweet', 'icecream', 'more', 'sweet'],\n",
       " ['chocolate', 'icecream', 'taste', 'more', 'delicious', 'banana']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Parameters of the model to Tune'''\n",
    "\n",
    "########################\n",
    "########################\n",
    "########################\n",
    "\n",
    "ALPHA = 0.2 # In per document the topic distribution, the higher the docs will have more topic\n",
    "BETA = 0.2 # per topic word distribution, the higher the topics will have more words\n",
    "ITERATIONS = 2000 # Go large, go !\n",
    "K = 2  # number of topics, a lot of time need to experiment this\n",
    "########################\n",
    "########################\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 unique words \n",
      "\n",
      "There are 12  documents \n",
      "\n",
      "We choose 2 topics \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initialize some intermediate storages and some latent parameters\n",
    "\n",
    "'''\n",
    "# Unique words list:\n",
    "word2id = list(set([j for i in data for j in i]))\n",
    "N = len(word2id)\n",
    "word2id = {j:i for i,j in enumerate(word2id)}\n",
    "print(\"There are %d unique words \\n\"%N)\n",
    "\n",
    "# M documents:\n",
    "M = len(data)\n",
    "print(\"There are %d  documents \\n\"%M)\n",
    "\n",
    "print(\"We choose %d topics \\n\"%K)\n",
    "\n",
    "\n",
    "def docmap(x_list):\n",
    "    return [word2id[w] for w in x_list]\n",
    "doc2id = [docmap(doc) for doc in data] # map data to lists of word indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Important Matrices Initialization\n",
    "\n",
    "Those 2 matrices will also be our output\n",
    "\n",
    "We will randomly assign topic to a word and use that to update each matrix\n",
    "'''\n",
    "\n",
    "DocTopic_mat = np.zeros((M,K)) \n",
    "WordTopic_mat = np.zeros((N,K)) \n",
    "topic_count_mat = [[0 for idx in doc] for doc in doc2id] # this list records assignment of each doc element to topics\n",
    "\n",
    "\n",
    "for _doc_id in range(M):\n",
    "    _tempDoc = doc2id[_doc_id] \n",
    "    for idx in range(len(_tempDoc)):\n",
    "        _word_id = _tempDoc[idx]\n",
    "        _random_topic = np.random.choice(range(K))\n",
    "        # Update each table:\n",
    "        topic_count_mat[_doc_id][idx] = _random_topic\n",
    "        DocTopic_mat[_doc_id,_random_topic] += 1\n",
    "        WordTopic_mat[_word_id,_random_topic] += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data preparation is done, we can start our modeling process. We will use **Gibbs Sampling** approach to continuously improve the topic assignment to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-0 started...\n",
      "Iteration-200 started...\n",
      "Iteration-400 started...\n",
      "Iteration-600 started...\n",
      "Iteration-800 started...\n",
      "Iteration-1000 started...\n",
      "Iteration-1200 started...\n",
      "Iteration-1400 started...\n",
      "Iteration-1600 started...\n",
      "Iteration-1800 started...\n"
     ]
    }
   ],
   "source": [
    "for i in range(ITERATIONS): # Iterations\n",
    "    if i % 200 == 0:\n",
    "        print(\"Iteration-%d started...\"%i)\n",
    "    \n",
    "\n",
    "    for _doc_id in range(M): # Each doc:\n",
    "        _temp_doc = doc2id[_doc_id]\n",
    "        for idx in range(len(_temp_doc)): # each word in doc d\n",
    "            # get word\n",
    "            _temp_word_idx = _temp_doc[idx]\n",
    "            # get topic\n",
    "            _temp_topic_idx = topic_count_mat[_doc_id][idx]\n",
    "\n",
    "\n",
    "            \n",
    "            # Pre-exclude current word:\n",
    "            WordTopic_mat[_temp_word_idx,_temp_topic_idx] -= 1\n",
    "            DocTopic_mat[_doc_id,_temp_topic_idx] -= 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Update using Gibbs sampling:\n",
    " \n",
    "                      # current word's topic assignment                # sum of all words count of each topic\n",
    "            phi_k_w= (WordTopic_mat[_temp_word_idx,:] + BETA)  /  (np.sum(WordTopic_mat,axis=0) + N*BETA) # phi\n",
    "                      # current doc's topic assignment                      # sum of all doc count of each topic\n",
    "            theta_m_k = (DocTopic_mat[_doc_id,:] + ALPHA)/ (np.sum(DocTopic_mat[_doc_id,:],axis=0) + ALPHA*K ) # theta\n",
    "            p = phi_k_w*theta_m_k\n",
    "\n",
    "            # normalize the p to sum up to 1 the allow next step\n",
    "            p = p/np.sum(p)\n",
    "            # get the new topic assignment:\n",
    "            new_topic = np.random.choice(range(K),p=p)\n",
    "           \n",
    "\n",
    "            WordTopic_mat[_temp_word_idx,new_topic] += 1\n",
    "            \n",
    "\n",
    "            DocTopic_mat[_doc_id,new_topic] += 1\n",
    "            \n",
    "            topic_count_mat[_doc_id][idx] = new_topic\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same approach above, now we have the P of each word assign to each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_res = (WordTopic_mat + BETA) / (np.sum(WordTopic_mat,axis=0) + N*BETA) # aka phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_res = (DocTopic_mat + ALPHA)/ (np.sum(DocTopic_mat,axis=0) + ALPHA*K ) # aka theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's check the results:\n",
    "\n",
    "Top 5 words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0044843 , 0.0044843 , 0.0044843 , 0.0044843 , 0.02690583,\n",
       "       0.0044843 , 0.16143498, 0.0044843 , 0.04932735, 0.07174888])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_res[:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 top words:\n",
      "['banana', 'icecream', 'food', 'delicious', 'apple']\n",
      "Topic 1 top words:\n",
      "['game', 'play', 'video', 'blizzard', 'starcraft']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id2word = {i[1]:i[0]  for i in word2id.items()}\n",
    "for i in range(K):\n",
    "    idxs = [i for i in reversed(words_res[:,i].argsort())][:5] # max -> min\n",
    "    print(\"Topic %d top words:\"%i)\n",
    "    print([id2word[idx] for idx in idxs])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sense right?\n",
    "\n",
    "#### the first topic is about video games !!\n",
    "#### the second on is about food  <3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic for each doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'delicious', 'food'],\n",
       " ['video', 'game', 'play', 'game', 'studio'],\n",
       " ['lunch', 'food', 'fruit', 'apple', 'banana', 'icecream'],\n",
       " ['warcraft', 'starcraft', 'overwatch', 'game'],\n",
       " ['chocolate', 'banana', 'icecream', 'most', 'delicious', 'food'],\n",
       " ['banana', 'apple', 'smoothie', 'lunch', 'dinner'],\n",
       " ['video', 'game', 'good', 'geeks'],\n",
       " ['what', 'eat', 'dinner', 'banana', 'chocolate'],\n",
       " ['which', 'game', 'company', 'better', 'ubisoft', 'blizzard'],\n",
       " ['play', 'game', 'ps4', 'xbox'],\n",
       " ['banana', 'less', 'sweet', 'icecream', 'more', 'sweet'],\n",
       " ['chocolate', 'icecream', 'taste', 'more', 'delicious', 'banana']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Data:\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_res.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again it's correctly assign each doc to the right topic:\n",
    "\n",
    "the ones are food related, the zeros are game related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================\n",
    "\n",
    "## Now let's put every together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \"\"\"\n",
    "    Latent Dirichlet Allocation using Gibbs Sampling.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ALPHA,\n",
    "        BETA,\n",
    "        ITERATIONS,\n",
    "        N_TOPICS,\n",
    "        verbose=True\n",
    "    ):\n",
    "        self.ALPHA = ALPHA\n",
    "        self.BETA = BETA\n",
    "        self.ITERATIONS = ITERATIONS\n",
    "        self.N_TOPICS = N_TOPICS\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    \n",
    "    def _preprocess_text(\n",
    "        self,\n",
    "        list_x,\n",
    "        _stopwords = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input: a list of strings (documents) without punctuations\n",
    "        will do some simple processing includes moving white spaces and stopwords\n",
    "        \"\"\"\n",
    "#         stopwords = ['to','or','is','the','and']\n",
    "        list_x = [doc.lower().split(' ') for doc in list_x]\n",
    "        list_x = [[i for i in doc if i!=''] for doc in list_x]\n",
    "        list_x = [[i for i in doc if i not in _stopwords] for doc in list_x]\n",
    "        self.raw_data = list_x\n",
    "\n",
    "\n",
    "    \n",
    "    def _initialize(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize latent varaibles and matrices\n",
    "        \"\"\"\n",
    "\n",
    "        # Unique words list:\n",
    "        word2id = list(set([j for i in self.raw_data for j in i]))\n",
    "        self.N = len(word2id)\n",
    "        self.word2id = {j:i for i,j in enumerate(word2id)}\n",
    "        self.id2word = {i[1]:i[0]  for i in self.word2id.items()}\n",
    "        # M documents:\n",
    "        self.M = len(self.raw_data)\n",
    "        self.doc2id = [self._docmap(doc) for doc in self.raw_data] # map data to lists of word indexes\n",
    "\n",
    "\n",
    "        \n",
    "        #Mat init:\n",
    "        self.DocTopic_mat = np.zeros((self.M,self.N_TOPICS)) \n",
    "        self.WordTopic_mat = np.zeros((self.N,self.N_TOPICS)) \n",
    "        self.topic_count_mat = [[0 for idx in doc] for doc in self.doc2id] # this list records assignment of each doc element to topics\n",
    "        \n",
    "        for _doc_id in range(self.M):\n",
    "            _tempDoc = self.doc2id[_doc_id] \n",
    "            for idx in range(len(_tempDoc)):\n",
    "                _word_id = _tempDoc[idx]\n",
    "                _random_topic = np.random.choice(range(self.N_TOPICS))\n",
    "                # Update each table:\n",
    "                self.topic_count_mat[_doc_id][idx] = _random_topic\n",
    "                self.DocTopic_mat[_doc_id,_random_topic] += 1\n",
    "                self.WordTopic_mat[_word_id,_random_topic] += 1\n",
    "\n",
    "\n",
    "    \n",
    "    def _docmap(self, x):\n",
    "        \"\"\"\n",
    "        Map list of lists of words to their id,\n",
    "        a.k.a Tokenization\n",
    "        \"\"\"\n",
    "        return [self.word2id[w] for w in x]\n",
    "     \n",
    "    \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Actual training using Gibbs Sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.ITERATIONS): # Iterations\n",
    "            if self.verbose:\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Iteration-%d started...\"%i)\n",
    "\n",
    "\n",
    "            for _doc_id in range(self.M): # Each doc:\n",
    "                _temp_doc = self.doc2id[_doc_id]\n",
    "                for idx in range(len(_temp_doc)): # each word in doc d\n",
    "                    # get word\n",
    "                    _temp_word_idx = _temp_doc[idx]\n",
    "                    # get topic\n",
    "                    _temp_topic_idx = self.topic_count_mat[_doc_id][idx]\n",
    "\n",
    "\n",
    "\n",
    "                    # Pre-exclude current word:\n",
    "                    self.WordTopic_mat[_temp_word_idx,_temp_topic_idx] -= 1\n",
    "                    self.DocTopic_mat[_doc_id,_temp_topic_idx] -= 1\n",
    "\n",
    "\n",
    "\n",
    "                    # Update using Gibbs sampling:\n",
    "\n",
    "                               # current word's topic assignment                # sum of all words count of each topic\n",
    "                    phi_k_w= (self.WordTopic_mat[_temp_word_idx,:] + self.BETA)  /  (np.sum(self.WordTopic_mat,axis=0) + self.N*self.BETA) # phi\n",
    "                            # current doc's topic assignment                      # sum of all doc count of each topic\n",
    "                    theta_m_k = (self.DocTopic_mat[_doc_id,:] + self.ALPHA)/ (np.sum(self.DocTopic_mat[_doc_id,:],axis=0) + self.ALPHA*self.N_TOPICS ) # theta\n",
    "                    p = phi_k_w*theta_m_k\n",
    "                    # normalize the p to sum up to 1 the allow next step\n",
    "                    p = p/np.sum(p)\n",
    "                    # get the new topic assignment:\n",
    "                    new_topic = np.random.choice(range(self.N_TOPICS),p=p)\n",
    "\n",
    "\n",
    "                    self.WordTopic_mat[_temp_word_idx,new_topic] += 1\n",
    "\n",
    "\n",
    "                    self.DocTopic_mat[_doc_id,new_topic] += 1\n",
    "\n",
    "                    self.topic_count_mat[_doc_id][idx] = new_topic\n",
    "\n",
    "        \n",
    "        self.res_wordtopic = (self.WordTopic_mat + self.BETA) / (np.sum(self.WordTopic_mat,axis=0) + self.N*self.BETA) # aka phi\n",
    "        self.res_doctopic = (self.DocTopic_mat + self.ALPHA)/ (np.sum(self.DocTopic_mat,axis=0) + self.ALPHA*self.N_TOPICS ) # aka theta\n",
    "\n",
    "   \n",
    "    def fit(\n",
    "        self,\n",
    "        data,\n",
    "        stopwords\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrap-up function to run the pipeline\n",
    "        \"\"\"\n",
    "        self._preprocess_text(data,stopwords)\n",
    "        self._initialize()\n",
    "        self._train()\n",
    "        \n",
    "\n",
    "    def get_topic_keywords(self,TOPIC,TOP_N):\n",
    "        \"\"\"\n",
    "        Query Top N keywords for certain topic\n",
    "        \"\"\"\n",
    "        idxs = [i for i in reversed(self.res_wordtopic[:,TOPIC].argsort())][:TOP_N] # max -> min\n",
    "        return [self.id2word[idx] for idx in idxs]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    'apple banana are delicious food',\n",
    "    'video game go play in game studio',\n",
    "    'lunch food is fruit apple banana icecream',\n",
    "    'warcraft or starcraft or overwatch best game',\n",
    "    'chocolate or banana or icecream the most delicious food',\n",
    "    'banana apple smoothie is  best for lunch or dinner',\n",
    "    'video game is good for geeks',\n",
    "    'what to eat for dinner banana or chocolate',\n",
    "    'which game company is better ubisoft or blizzard',\n",
    "    'play game on ps4 or xbox',\n",
    "    'banana is less sweet icecream is more sweet',\n",
    "    'chocolate icecream taste more delicious than banana'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['to','or','is','the','and','in','for','are','on','go','best','than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.2 # In per document the topic distribution, the higher the docs will havemore topic\n",
    "BETA = 0.2 # per topic word distribution, the higher the topics will have more words\n",
    "ITERATIONS = 1500 # Go large, go !\n",
    "K = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LDA(\n",
    "    ALPHA=ALPHA,\n",
    "    BETA=BETA,\n",
    "    ITERATIONS = ITERATIONS,\n",
    "    N_TOPICS = K,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration-0 started...\n",
      "Iteration-100 started...\n",
      "Iteration-200 started...\n",
      "Iteration-300 started...\n",
      "Iteration-400 started...\n",
      "Iteration-500 started...\n",
      "Iteration-600 started...\n",
      "Iteration-700 started...\n",
      "Iteration-800 started...\n",
      "Iteration-900 started...\n",
      "Iteration-1000 started...\n",
      "Iteration-1100 started...\n",
      "Iteration-1200 started...\n",
      "Iteration-1300 started...\n",
      "Iteration-1400 started...\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    data=data,\n",
    "    stopwords = stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game', 'play', 'video', 'blizzard', 'starcraft']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_keywords(TOPIC=0,TOP_N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['banana', 'icecream', 'food', 'delicious', 'apple']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_keywords(TOPIC=1,TOP_N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Done --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
