{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(X,idx2word):\n",
    "    ncol = len(idx2word.keys())\n",
    "    nrow = len(X)\n",
    "    OHE_X = np.zeros((nrow,ncol))\n",
    "    for r in range(len(X)):\n",
    "        if not isinstance(X[r],list):\n",
    "            OHE_X[r,X[r]] = 1\n",
    "        else:\n",
    "            row_val = X[r]\n",
    "            for c in row_val:\n",
    "                OHE_X[r,c] = 1\n",
    "                \n",
    "    return OHE_X\n",
    "        \n",
    "\n",
    "    \n",
    "def tokenize(x_list):\n",
    "    #unique tokens:\n",
    "    unique_x = list(set([j for i in data for j in i]))\n",
    "    idx2word = dict(enumerate(unique_x))\n",
    "    word2idx = {i[1]:i[0] for i in idx2word.items()}\n",
    "    # Encode:\n",
    "    tokened_x_list = []\n",
    "    for sentence in x_list:\n",
    "        temp_sent = []\n",
    "        for word in sentence:\n",
    "            token = word2idx.get(word,-1)\n",
    "            temp_sent.append(token)\n",
    "        \n",
    "        tokened_x_list.append(temp_sent)\n",
    "    return tokened_x_list,idx2word,word2idx\n",
    "            \n",
    "\n",
    "    \n",
    "def skipgram_prep(x_list,context_window=2):\n",
    "    \"\"\"\n",
    "    Use Skipgram method to prepare the data.\n",
    "    \n",
    "    Arguments:\n",
    "        x_list(list): tokenized training data\n",
    "        \n",
    "        context_window: the context window on each side. \n",
    "        For example, if context_window=2, we will be looking at 2 tokens on the left and \n",
    "        2 tokens on the right\n",
    "    \n",
    "    Returns:\n",
    "        processd_data(list):  a list of tuples represents the processed data. Each pair of tuple is a (x,y) pair\n",
    "        \n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for row in x_list:\n",
    "        row_len = len(row)\n",
    "        for i in range(row_len):\n",
    "            x = row[i]\n",
    "            start_idx = max(i-context_window,0)\n",
    "            end_idx = min(row_len,i+context_window+1)\n",
    "            y = row[start_idx:i] + row[i+1:end_idx] # skip the self\n",
    "            \n",
    "            \n",
    "            temp_xy_pair = zip([x]*len(y),y)\n",
    "            processed_data.extend(temp_xy_pair)\n",
    "    \n",
    "    return processed_data\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return np.divide(e_x,e_x.sum(axis=1).reshape(-1,1))\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    'apple and banana are delicious food',\n",
    "    'play video game in game studio',\n",
    "    'i have orc warrior in world of warcraft',\n",
    "    'super mario is the best video game',\n",
    "    'do you prefer xbox or ps4 or nintentdo switch',\n",
    "    'which game company is better ubisoft or blizzard',\n",
    "    'play game on ps4 or xbox',\n",
    "    'video game is exciting for geeks',\n",
    "    'warcraft or starcraft or overwatch is best game',\n",
    "    'is warcraft and world of warcraft the same game',\n",
    "    'i prefer desktop game console to either xbox or ps4',\n",
    "    \n",
    "    \n",
    "    'food for lunch is fruit apple banana icecream',\n",
    "    'icecream is too sweet for a meal',\n",
    "    'dove chocolate is my favorite',\n",
    "    'which is sweet chocolate or icecream',\n",
    "    'chocolate or banana or icecream is the most delicious food',\n",
    "    'go to buy a banana bread for my lunch',\n",
    "    'banana apple smoothie is the best for lunch or dinner',\n",
    "    'what to eat for dinner banana or chocolate',\n",
    "    'banana is less sweet icecream is more sweet',\n",
    "    'chocolate icecream taste more delicious than banana',\n",
    "    'chicken sandwich is different from chicken bread',\n",
    "    \n",
    "]\n",
    "\n",
    "data = [i.split(\" \") for i in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_list,idx2word,word2idx = tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'prefer', 1: 'starcraft', 2: 'a', 3: 'go', 4: 'warrior', 5: 'buy', 6: 'smoothie', 7: 'chicken', 8: 'sandwich', 9: 'xbox', 10: 'super', 11: 'mario', 12: 'to', 13: 'geeks', 14: 'the', 15: 'sweet', 16: 'fruit', 17: 'have', 18: 'console', 19: 'dove', 20: 'meal', 21: 'play', 22: 'on', 23: 'warcraft', 24: 'of', 25: 'ps4', 26: 'blizzard', 27: 'for', 28: 'exciting', 29: 'favorite', 30: 'same', 31: 'icecream', 32: 'eat', 33: 'more', 34: 'different', 35: 'desktop', 36: 'most', 37: 'from', 38: 'nintentdo', 39: 'orc', 40: 'in', 41: 'studio', 42: 'what', 43: 'are', 44: 'apple', 45: 'my', 46: 'or', 47: 'is', 48: 'either', 49: 'banana', 50: 'game', 51: 'too', 52: 'company', 53: 'taste', 54: 'ubisoft', 55: 'and', 56: 'best', 57: 'food', 58: 'lunch', 59: 'less', 60: 'bread', 61: 'delicious', 62: 'than', 63: 'dinner', 64: 'you', 65: 'switch', 66: 'video', 67: 'i', 68: 'overwatch', 69: 'which', 70: 'chocolate', 71: 'world', 72: 'do', 73: 'better'}\n"
     ]
    }
   ],
   "source": [
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Skipgram to Prepare the Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = skipgram_prep(tokenized_data_list,context_window=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i[0] for i in prep_data]\n",
    "Y = [i[1] for i in prep_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_X = ohe(X,idx2word)\n",
    "ohe_Y = ohe(Y,idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Word2vec Model:\n",
    "\n",
    "First, let's build a naive Word2vec model, means we're gonna use softmax across all vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters:\n",
    "LEARNING_RATE = 0.01\n",
    "N_VOCAB = len(idx2word)\n",
    "N_DIM = 10\n",
    "BATCH_SIZE = len(ohe_X)\n",
    "\n",
    "# Weights Initialization:\n",
    "embedding_mat = np.random.normal(size=(N_VOCAB,N_DIM)) \n",
    "dense_w = np.random.normal(size=(N_DIM,N_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.939235267658686\n",
      "Loss: 2.890509146952381\n",
      "Loss: 2.8084968702492077\n",
      "Loss: 2.784884496026286\n",
      "Loss: 2.777874359768054\n",
      "Loss: 2.773318426613309\n",
      "Loss: 2.7703245545204656\n",
      "Loss: 2.774656720494175\n",
      "Loss: 2.7635576135418134\n",
      "Loss: 2.766479310088463\n",
      "Loss: 2.7683019537472418\n",
      "Loss: 2.7595713581987953\n",
      "Loss: 2.7667158065504105\n",
      "Loss: 2.764545418555038\n",
      "Loss: 2.7580792021073135\n"
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "for i in range(1500):\n",
    "    \n",
    "\n",
    "    # forward pass:\n",
    "    input_x = ohe_X\n",
    "    input_y = ohe_Y\n",
    "    x_embedding_layer = input_x.dot(embedding_mat)# query word embedding X\n",
    "#     print(x_embedding_layer.shape)\n",
    "    dense_layer = x_embedding_layer.dot(dense_w)\n",
    "#     print(dense_layer.shape)\n",
    "    output_layer = softmax(dense_layer)\n",
    "\n",
    "    # cross entropy loss:\n",
    "    loss = -np.sum(input_y*np.log(output_layer+1e-9))/BATCH_SIZE # adding smooth term\n",
    "    if i%100==0:\n",
    "        print(f\"Loss: {loss}\")\n",
    "    all_loss.append(loss)\n",
    "#     print('---')\n",
    "    \n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    \n",
    "\n",
    "    # d_loss/d_dense_layer = d_loss/d_op_layer * d_op_layer/d_dense_layer\n",
    "    d_dense = output_layer - input_y\n",
    "#     print(d_dense.shape)\n",
    "\n",
    "    # d_loss/d_dense_w = d_loss/d_dense_layer * d_dense_layer/d_dense_w\n",
    "    d_dense_w =  d_dense.T.dot(x_embedding_layer).T\n",
    "#     print(d_dense_w.shape)\n",
    "\n",
    "    # d_loss/x_embedding_layer = d_loss/d_dense_layer * d_dense_layer/x_embedding_layer\n",
    "    d_emb_layer =  d_dense.dot(dense_w.T)\n",
    "#     print(d_emb_layer.shape)\n",
    "    # d_loss/d_embedding_mat = d_loss/x_embedding_layer * x_embedding_layer/d_embedding_mat\n",
    "    d_embedding_mat = d_emb_layer.T.dot(input_x)\n",
    "#     print(d_embedding_mat.shape)\n",
    "#     print('~')\n",
    "\n",
    "\n",
    "    \n",
    "    embedding_mat -= LEARNING_RATE*d_embedding_mat.T\n",
    "    dense_w -= LEARNING_RATE*d_dense_w\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the mini word2vec model is ready, let build the query function to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word ,embedding = embedding_mat,word2idx=word2idx,vector_dim=N_DIM):\n",
    "    \n",
    "    query_id = word2idx.get(word,-1)\n",
    "    if query_id>=0:\n",
    "        return embedding_mat[query_id,:]\n",
    "    else:\n",
    "        return np.zeros((N_DIM,))-999.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.76500087,  1.07809326, -1.61388524,  0.63570399, -0.25055495,\n",
       "       -0.42878406, -0.20876328, -0.83644503,  1.39433293, -0.60998938])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with in vocab word:\n",
    "query_word = 'xbox'\n",
    "get_word_vector(query_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-999., -999., -999., -999., -999., -999., -999., -999., -999.,\n",
       "       -999.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with in Out-of_vocabulary word:\n",
    "query_word = 'lol'\n",
    "get_word_vector(query_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find the most similar word to our query word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_sim(vx,vy):\n",
    "    return dot(vx, vy)/(norm(vx)*norm(vy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(query_word,word2idx=word2idx):\n",
    "    query_vector = get_word_vector(query_word)\n",
    "    \n",
    "    result = {}\n",
    "    for word in word2idx:\n",
    "        temp_vector = get_word_vector(word)\n",
    "#         print(word)\n",
    "#         print(temp_vector)\n",
    "        sim = cosine_sim(query_vector,temp_vector)\n",
    "        result[word] = sim\n",
    "    \n",
    "    return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 1.0000000000000002),\n",
       " ('delicious', 0.799574238439338),\n",
       " ('banana', 0.696540293747587)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = find_similar('food')\n",
    "\n",
    "\n",
    "sorted(list(result.items()),key=lambda x: x[1],reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warcraft', 1.0),\n",
       " ('world', 0.7461912120209175),\n",
       " ('starcraft', 0.7454474518475237)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = find_similar('warcraft')\n",
    "\n",
    "\n",
    "sorted(list(result.items()),key=lambda x: x[1],reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 0.9999999999999998),\n",
       " ('video', 0.7487766536647733),\n",
       " ('overwatch', 0.7384422164865021)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = find_similar('game')\n",
    "\n",
    "\n",
    "sorted(list(result.items()),key=lambda x: x[1],reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xbox', 1.0000000000000002),\n",
       " ('i', 0.7077697803104328),\n",
       " ('ps4', 0.6796021254332196)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result = find_similar('xbox')\n",
    "\n",
    "\n",
    "sorted(list(result.items()),key=lambda x: x[1],reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what's the problem here -- the above algorithm will never work for real world problem!!!\n",
    "\n",
    "We only have 44 vocabularies in this vanilla example. What if we have millions of vocabs? The softmax operation becomes very expensive. To tackle this  issue, several algorithms are proposed, in order to do the approximation of softmax, such as: Hiearchical Softmax, Negative Sampling or NCE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Negative Sampling helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_negative(xy_pairs,n_negative,idx2word):\n",
    "\n",
    "    pos_context = {}\n",
    "    grand_negative_samples = []\n",
    "    for x,y in xy_pairs:\n",
    "\n",
    "        if x not in pos_context:\n",
    "            good_pair = [i[1] for i in xy_pairs if i[0]==x]\n",
    "            pos_context[x] = good_pair\n",
    "\n",
    "        ## Sample:\n",
    "        temp_neg_samples = []\n",
    "        while len(temp_neg_samples)< n_negative:\n",
    "            temp_idx = np.random.choice(list(idx2word.keys()))\n",
    "            if temp_idx!=x and temp_idx not in pos_context[x]:\n",
    "                temp_neg_samples.append(temp_idx)\n",
    "\n",
    "        grand_negative_samples.append(temp_neg_samples)\n",
    "    return np.array(grand_negative_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEGATIVE_WORD = 3\n",
    "LEARNING_RATE = 0.01\n",
    "N_VOCAB = len(idx2word)\n",
    "N_DIM = 10\n",
    "BATCH_SIZE = len(ohe_X)\n",
    "\n",
    "# Weights Initialization:\n",
    "embedding_mat = np.random.normal(size=(N_VOCAB,N_DIM)) \n",
    "dense_w = np.random.normal(size=(N_DIM,N_VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_X = ohe(X,idx2word)\n",
    "ohe_Y = ohe(Y,idx2word)\n",
    "\n",
    "pos_words_mask = ohe_Y.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec Model with Negative Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mask above to optimize only the positive word and the negative words we sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.134581618402228\n",
      "Loss: 7.390820574127913\n",
      "Loss: 7.146451696102426\n",
      "Loss: 7.991370780662379\n",
      "Loss: 9.94129644357155\n",
      "Loss: 12.105866663919734\n",
      "Loss: 15.110796887869324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/algo_dev/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n",
      "Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-d1bca179576a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxy_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_negative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_NEGATIVE_WORD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     idx2word = idx2word)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mneg_words_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-26ff387b7a87>\u001b[0m in \u001b[0;36msample_negative\u001b[0;34m(xy_pairs, n_negative, idx2word)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_neg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mn_negative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtemp_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtemp_idx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtemp_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_context\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mtemp_neg_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "for i in range(1000):\n",
    "    neg_words = sample_negative(\n",
    "    xy_pairs=prep_data,\n",
    "    n_negative=N_NEGATIVE_WORD,\n",
    "    idx2word = idx2word)\n",
    "\n",
    "    neg_words_mask = ohe(neg_words.tolist(),idx2word)\n",
    "    all_mask = pos_words_mask+neg_words_mask\n",
    "\n",
    "    \n",
    "\n",
    "    # forward pass:\n",
    "    input_x = ohe_X\n",
    "    input_y = ohe_Y\n",
    "    x_embedding_layer = input_x.dot(embedding_mat)# query word embedding X\n",
    "    dense_layer = x_embedding_layer.dot(dense_w)\n",
    "    output_layer = softmax(dense_layer)\n",
    "\n",
    "    output_layer = output_layer*all_mask\n",
    "    # cross entropy loss:\n",
    "#     print(output_layer)\n",
    "    loss = -np.sum(input_y*np.log(output_layer+1e-9))/BATCH_SIZE # adding smooth term\n",
    "    if i%2==0:\n",
    "        print(f\"Loss: {loss}\")\n",
    "    all_loss.append(loss)\n",
    "#     print('---')\n",
    "    \n",
    "\n",
    "\n",
    "    # Backward Pass\n",
    "    \n",
    "\n",
    "    # d_loss/d_dense_layer = d_loss/d_op_layer * d_op_layer/d_dense_layer\n",
    "    d_dense = output_layer - input_y\n",
    "#     print(d_dense.shape)\n",
    "\n",
    "    # d_loss/d_dense_w = d_loss/d_dense_layer * d_dense_layer/d_dense_w\n",
    "    d_dense_w =  d_dense.T.dot(x_embedding_layer).T\n",
    "#     print(d_dense_w.shape)\n",
    "\n",
    "    # d_loss/x_embedding_layer = d_loss/d_dense_layer * d_dense_layer/x_embedding_layer\n",
    "    d_emb_layer =  d_dense.dot(dense_w.T)\n",
    "#     print(d_emb_layer.shape)\n",
    "\n",
    "    # d_loss/d_embedding_mat = d_loss/x_embedding_layer * x_embedding_layer/d_embedding_mat\n",
    "    d_embedding_mat = d_emb_layer.T.dot(input_x)\n",
    "#     print(d_embedding_mat.shape)\n",
    "#     print('~')\n",
    "\n",
    "\n",
    "    \n",
    "    embedding_mat -= LEARNING_RATE*d_embedding_mat.T\n",
    "    dense_w -= LEARNING_RATE*d_dense_w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
