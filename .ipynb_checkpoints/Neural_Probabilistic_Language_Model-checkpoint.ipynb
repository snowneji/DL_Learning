{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently I was reading the paper: **A Neural Probabilistic Language Model** from Yoshua Bengio: http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf, which is the milestone of using Neural Network for Language Model. Traditionally, we multiply the chain of conditional probablity to estimate the probablity of the next word: **Pˆ(w1)=∏Pˆ(wt|w1 )**. However, the problem is in real world text, we got hundreds and thousands of words, which makes our calculation very expensive. One work around is to apply **n-gram** technique, which is basically just to estimate the probability using recent n words. What Yoshua Bengio did is using a neural network to build the language model, I believe later work such as **Word2vec** has some similar spirit to this work, so I decided to build  NNLM from scratch to understand it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper the overall approach is as the follows:\n",
    "\n",
    "1. associate with each word in the vocabulary a distributed word feature vector (a real- valued vector in Rm),\n",
    "2. express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence, and\n",
    "3. learn simultaneously the word feature vectors and the parameters of that probability function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to get the next word unnormalized log probablity of each word, we simply calculate the following :\n",
    "    \n",
    "    \n",
    "    y = b + W x + U tanh(d + H x)\n",
    "    \n",
    "    \n",
    "in which the weight matrices:H,U,W and the biases: b,d are learned from the neural network.\n",
    "After we get y, we find the normalized probability using through `softmax(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oheY(Y,idx2word):\n",
    "    ncol = len(idx2word.keys())\n",
    "    nrow = len(Y)\n",
    "    OHE_Y = np.zeros((nrow,ncol))\n",
    "    for r in range(len(Y)):\n",
    "        if not isinstance(Y[r],list):\n",
    "            OHE_Y[r,Y[r]] = 1\n",
    "        else:\n",
    "            row_val = Y[r]\n",
    "            for c in row_val:\n",
    "                OHE_Y[r,c] = 1\n",
    "                \n",
    "    return OHE_Y\n",
    "        \n",
    "\n",
    "    \n",
    "def tokenize(x_list,word2idx,START_TOKEN,END_TOKEN):\n",
    "    #unique tokens:\n",
    "    unique_x = list(set([j for i in x_list for j in i]))\n",
    "    \n",
    "    for w in unique_x:\n",
    "        if w not in word2idx:\n",
    "            word2idx[w] = len(word2idx)\n",
    "    \n",
    "    \n",
    "        \n",
    "    idx2word = {i[1]:i[0] for i in word2idx.items()}\n",
    "    # Encode:\n",
    "    tokened_x_list = []\n",
    "    for sentence in x_list:\n",
    "        temp_sent = []\n",
    "        for word in sentence:\n",
    "            token = word2idx.get(word,-1)\n",
    "            temp_sent.append(token)\n",
    "        \n",
    "        tokened_x_list.append(temp_sent)\n",
    "    return tokened_x_list,idx2word,word2idx\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return np.divide(e_x,e_x.sum(axis=1).reshape(-1,1))\n",
    "            \n",
    "            \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def training_data_prep(x_list,n):\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    Arguments:\n",
    "        x_list(list): tokenized training data\n",
    "        \n",
    "        n: numbers of context word to look at to predict the next word\n",
    "    \n",
    "    Returns:\n",
    "        processd_data(list):  a list of tuples represents the processed data. Each pair of tuple is a (x,y) pair\n",
    "        \n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for row in x_list:\n",
    "        row_len = len(row)\n",
    "        for i in range(n,row_len):\n",
    "       \n",
    "\n",
    "            X.append(row[(i-n):i])\n",
    "            Y.append(row[i])\n",
    "\n",
    "     \n",
    "    \n",
    "    return X,Y\n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def tanh(x,if_derivative=False):\n",
    "    \n",
    "    if if_derivative:\n",
    "        return 1 - np.tanh(x)*np.tanh(x)\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def cross_entropy(y,y_hat):\n",
    "    return -np.sum(y*np.log(y_hat+1e-9))/len(y)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def text_clean(x_string):\n",
    "    x_string = x_string.lower()\n",
    "    x_string = re.sub(r'[\\w\\.-]+@[\\w\\.-]+(\\.[\\w]+)+','',x_string)\n",
    "    x_string = x_string.replace(\"\\n\",\" \")\n",
    "    x_string = x_string.replace(\"\\t\",\" \")\n",
    "\n",
    "    x_string = re.sub(r'[^\\s\\w_]+',' ',x_string)\n",
    "\n",
    "    return x_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    'apple and banana are delicious food',\n",
    "    'I love play video game in game studio',\n",
    "    'i have orc warrior in world of warcraft',\n",
    "    'super mario is the best video game ever',\n",
    "    'is warcraft and world of warcraft the same game',\n",
    "    'icecream is too sweet for a meal',\n",
    "    'dove chocolate is my favorite',\n",
    "    'which is sweet chocolate or icecream',\n",
    "    'chocolate or banana or icecream is the most delicious food',\n",
    "    'go to buy a banana bread for my lunch',\n",
    "    'banana apple smoothie is the best for lunch',\n",
    "    'chocolate icecream taste more delicious than banana',\n",
    "    'chicken sandwich is different from chicken bread',\n",
    "    'The present tense is more common in writing', \n",
    "    'As you can see by all the explanations I give', \n",
    "    'Instructional writing is more commonly written in present tense',\n",
    "    'I want to make sure everyone is ready for the field trip next week',\n",
    "    'I find it rather difficult to pick one that leaves me with the fabled',\n",
    "    'I had the pleasure of traveling across America in many moving trips',\n",
    "    'The blazing, red, diesel fuel tanks beamed in front of the station, looking like cheap lipstick',\n",
    "    'This family was a victim of a problem they could have avoided',\n",
    "    'Do you think that schools should track students with tracking technology',\n",
    "    'im writing an essay write now and im getting super confused',\n",
    "    'Despite heading the Forbes list, Messi has endured a mixed season at the Catalan club',\n",
    "    'The move comes ahead of a vote in the committee',\n",
    "    'At least one federal court in a different, but related case, agreed with the administration',\n",
    "    'tomorrow evening is probably a better time to work on the assignment',\n",
    "    'do you think computer will replace human in the future',\n",
    "    'I think school is the worst place'\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [text_clean(sent) for sent in data]\n",
    "data = [sent.split(\" \") for sent in data]\n",
    "data = [[word for word in sent if len(word)>1] for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing:\n",
    "\n",
    "For the preprocessing step, we add `start_token` and `end_token` and for the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START_TOKEN = '^'\n",
    "# END_TOKEN = '$'\n",
    "\n",
    "# data = [[START_TOKEN]+sent+[END_TOKEN] for sent in data]\n",
    "\n",
    "# print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dict = {START_TOKEN:0,END_TOKEN:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_list,idx2word,word2idx = tokenize(data,init_dict,START_TOKEN,END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^': 0, '$': 1, 'by': 2, 'now': 3, 'leaves': 4, 'across': 5, 'meal': 6, 'most': 7, 'in': 8, 'court': 9, 'all': 10, 'see': 11, 'ahead': 12, 'blazing': 13, 'or': 14, 'technology': 15, 'of': 16, 'front': 17, 'replace': 18, 'list': 19, 'trips': 20, 'time': 21, 'orc': 22, 'im': 23, 'banana': 24, 'avoided': 25, 'dove': 26, 'trip': 27, 'tracking': 28, 'human': 29, 'do': 30, 'place': 31, 'with': 32, 'sure': 33, 'catalan': 34, 'lunch': 35, 'warrior': 36, 'pick': 37, 'from': 38, 'but': 39, 'as': 40, 'related': 41, 'future': 42, 'my': 43, 'make': 44, 'favorite': 45, 'looking': 46, 'problem': 47, 'present': 48, 'delicious': 49, 'work': 50, 'that': 51, 'explanations': 52, 'for': 53, 'sandwich': 54, 'station': 55, 'tomorrow': 56, 'love': 57, 'students': 58, 'written': 59, 'pleasure': 60, 'difficult': 61, 'to': 62, 'taste': 63, 'rather': 64, 'vote': 65, 'commonly': 66, 'they': 67, 'traveling': 68, 'want': 69, 'cheap': 70, 'has': 71, 'schools': 72, 'different': 73, 'this': 74, 'the': 75, 'beamed': 76, 'tense': 77, 'red': 78, 'assignment': 79, 'committee': 80, 'fabled': 81, 'field': 82, 'think': 83, 'which': 84, 'season': 85, 'family': 86, 'forbes': 87, 'same': 88, 'it': 89, 'super': 90, 'had': 91, 'chicken': 92, 'studio': 93, 'are': 94, 'chocolate': 95, 'club': 96, 'sweet': 97, 'evening': 98, 'is': 99, 'ever': 100, 'mario': 101, 'tanks': 102, 'than': 103, 'go': 104, 'comes': 105, 'best': 106, 'one': 107, 'america': 108, 'messi': 109, 'move': 110, 'moving': 111, 'next': 112, 'federal': 113, 'better': 114, 'heading': 115, 'give': 116, 'least': 117, 'mixed': 118, 'worst': 119, 'despite': 120, 'essay': 121, 'track': 122, 'video': 123, 'writing': 124, 'computer': 125, 'everyone': 126, 'agreed': 127, 'game': 128, 'week': 129, 'many': 130, 'and': 131, 'getting': 132, 'an': 133, 'world': 134, 'probably': 135, 'school': 136, 'at': 137, 'can': 138, 'endured': 139, 'case': 140, 'smoothie': 141, 'ready': 142, 'have': 143, 'on': 144, 'find': 145, 'fuel': 146, 'bread': 147, 'you': 148, 'like': 149, 'victim': 150, 'play': 151, 'diesel': 152, 'common': 153, 'too': 154, 'write': 155, 'more': 156, 'me': 157, 'could': 158, 'administration': 159, 'lipstick': 160, 'confused': 161, 'will': 162, 'food': 163, 'apple': 164, 'warcraft': 165, 'was': 166, 'instructional': 167, 'should': 168, 'icecream': 169, 'buy': 170}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training Data:\n",
    "\n",
    "To prepare the training data, we use n words before the target word to predict it, so we have the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = training_data_prep(tokenized_data_list,n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'and', 'banana'] -> are\n",
      "['and', 'banana', 'are'] -> delicious\n",
      "['banana', 'are', 'delicious'] -> food\n",
      "['love', 'play', 'video'] -> game\n",
      "['play', 'video', 'game'] -> in\n",
      "['video', 'game', 'in'] -> game\n",
      "['game', 'in', 'game'] -> studio\n",
      "['have', 'orc', 'warrior'] -> in\n",
      "['orc', 'warrior', 'in'] -> world\n",
      "['warrior', 'in', 'world'] -> of\n",
      "['in', 'world', 'of'] -> warcraft\n",
      "['super', 'mario', 'is'] -> the\n",
      "['mario', 'is', 'the'] -> best\n",
      "['is', 'the', 'best'] -> video\n",
      "['the', 'best', 'video'] -> game\n",
      "['best', 'video', 'game'] -> ever\n",
      "['is', 'warcraft', 'and'] -> world\n",
      "['warcraft', 'and', 'world'] -> of\n",
      "['and', 'world', 'of'] -> warcraft\n",
      "['world', 'of', 'warcraft'] -> the\n",
      "['of', 'warcraft', 'the'] -> same\n",
      "['warcraft', 'the', 'same'] -> game\n",
      "['icecream', 'is', 'too'] -> sweet\n",
      "['is', 'too', 'sweet'] -> for\n",
      "['too', 'sweet', 'for'] -> meal\n",
      "['dove', 'chocolate', 'is'] -> my\n",
      "['chocolate', 'is', 'my'] -> favorite\n",
      "['which', 'is', 'sweet'] -> chocolate\n",
      "['is', 'sweet', 'chocolate'] -> or\n",
      "['sweet', 'chocolate', 'or'] -> icecream\n",
      "['chocolate', 'or', 'banana'] -> or\n",
      "['or', 'banana', 'or'] -> icecream\n",
      "['banana', 'or', 'icecream'] -> is\n",
      "['or', 'icecream', 'is'] -> the\n",
      "['icecream', 'is', 'the'] -> most\n",
      "['is', 'the', 'most'] -> delicious\n",
      "['the', 'most', 'delicious'] -> food\n",
      "['go', 'to', 'buy'] -> banana\n",
      "['to', 'buy', 'banana'] -> bread\n",
      "['buy', 'banana', 'bread'] -> for\n",
      "['banana', 'bread', 'for'] -> my\n",
      "['bread', 'for', 'my'] -> lunch\n",
      "['banana', 'apple', 'smoothie'] -> is\n",
      "['apple', 'smoothie', 'is'] -> the\n",
      "['smoothie', 'is', 'the'] -> best\n",
      "['is', 'the', 'best'] -> for\n",
      "['the', 'best', 'for'] -> lunch\n",
      "['chocolate', 'icecream', 'taste'] -> more\n",
      "['icecream', 'taste', 'more'] -> delicious\n",
      "['taste', 'more', 'delicious'] -> than\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    tmpx = [idx2word.get(idx) for idx in X[i]]\n",
    "    tmpy = idx2word.get(Y[i])\n",
    "    print(f\"{tmpx} -> {tmpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE:\n",
    "\n",
    "To feed into the network, we need to `one-hot-encode` our X and Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_Y = oheY(Y,idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sent in X:\n",
    "    X_train.append(oheY(sent,idx2word))\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's start the NN Architecture:\n",
    "\n",
    "now we have our data prepared for the model, note that we get rid of the biases for the simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X;ohe_Y;idx2word;word2idx;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(idx2word) # vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 16 # number of hiddent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5 #5 embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 300 #300\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "C = np.random.normal(size=(V,m)) # embedding weight matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "U = np.random.normal(size=(V,h)) # embedding weight matrix\n",
    "W = np.random.normal(size=(V,N*m   )) # embedding weight matrix\n",
    "\n",
    "H = np.random.normal(size=(h,N*m)) # Hidden Layer matrix \n",
    "\n",
    "d = np.zeros((1,h))\n",
    "\n",
    "b = np.zeros((1,V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss--14.056023024923679\n",
      "Loss--13.341058998834047\n",
      "Loss--12.632042007867355\n",
      "Loss--11.981858695556054\n",
      "Loss--11.429418374507307\n",
      "Loss--10.957535597872845\n",
      "Loss--10.52879492619196\n",
      "Loss--10.122839055458668\n",
      "Loss--9.743512316584301\n",
      "Loss--9.39730930438617\n",
      "Loss--9.083354037011866\n",
      "Loss--8.799498273915685\n",
      "Loss--8.539225107942102\n",
      "Loss--8.295292696900908\n",
      "Loss--8.063266827986105\n",
      "Loss--7.840909489726805\n",
      "Loss--7.627451476964967\n",
      "Loss--7.42363843856118\n",
      "Loss--7.230214871300546\n",
      "Loss--7.047177402633065\n",
      "Loss--6.873703961682445\n",
      "Loss--6.708297069763818\n",
      "Loss--6.548997746092417\n",
      "Loss--6.394100920816377\n",
      "Loss--6.242815088334415\n",
      "Loss--6.095622675208398\n",
      "Loss--5.95357357474589\n",
      "Loss--5.817165485727904\n",
      "Loss--5.686026236454435\n",
      "Loss--5.559475888367311\n",
      "Loss--5.4370423083014465\n",
      "Loss--5.318514128822914\n",
      "Loss--5.203802485932598\n",
      "Loss--5.092788992341444\n",
      "Loss--4.985239766868958\n",
      "Loss--4.880826580985282\n",
      "Loss--4.77920557343942\n",
      "Loss--4.6800870446229785\n",
      "Loss--4.583267438185829\n",
      "Loss--4.488598981549998\n",
      "Loss--4.39594083437299\n",
      "Loss--4.305191001409964\n",
      "Loss--4.216339203947551\n",
      "Loss--4.129407501801854\n",
      "Loss--4.044361946744393\n",
      "Loss--3.9611312427731566\n",
      "Loss--3.879658192432235\n",
      "Loss--3.7999001488439696\n",
      "Loss--3.7218046002592935\n",
      "Loss--3.645299352538926\n",
      "Loss--3.5702993541962025\n",
      "Loss--3.496717687670862\n",
      "Loss--3.4244752917902614\n",
      "Loss--3.3535097134182106\n",
      "Loss--3.2837823227967307\n",
      "Loss--3.215281004252819\n",
      "Loss--3.1480151813014317\n",
      "Loss--3.0820034085056323\n",
      "Loss--3.017258898616232\n",
      "Loss--2.9537798871507097\n"
     ]
    }
   ],
   "source": [
    "for iter in range(n_iter):\n",
    "    \n",
    "    # Forward Propagation:\n",
    "    x0 = X_train.dot(C) # query the word vector from Word Embedding Layer:  MxNxH\n",
    "    x = x0.reshape(x0.shape[0],-1) # Concat: Mx(N*H)\n",
    "    hx = x.dot(H.T) + d#Mxh\n",
    "    act_hx = tanh(hx,if_derivative=False)#Mxh\n",
    "    U_act_hx = act_hx.dot(U.T) # MxV\n",
    "    Wx = x.dot(W.T) #MxV\n",
    "    \n",
    "    add_all_x = b+Wx+U_act_hx\n",
    "    output = softmax(add_all_x)\n",
    "    loss = cross_entropy(ohe_Y,output)\n",
    "    LOSS.append(loss)\n",
    "    if iter%5==0:\n",
    "        print(f\"Loss--{loss}\")\n",
    "    \n",
    "    \n",
    "    # Backward Propagation\n",
    "\n",
    "    #d_loss/d_add_all_x: (I hacked this step), d of cross_entropy+softmax is the subtraction\n",
    "    d_add_all_x = output - ohe_Y\n",
    "    \n",
    "    \n",
    "    d_b = d_add_all_x.sum(axis=0,keepdims=True)\n",
    "    d_Wx = d_add_all_x.copy() # mxv\n",
    "    d_U_act_hx = d_add_all_x.copy()\n",
    "    \n",
    "    #d_loss/d_w\n",
    "    d_W = d_Wx.T.dot(x)\n",
    "    #d_loss/d_U\n",
    "    d_U = d_U_act_hx.T.dot(act_hx)\n",
    "    #d_loss/d_act_hx\n",
    "    d_act_hx = d_U_act_hx.dot(U)\n",
    "    #d_loss/d_hx\n",
    "    d_hx = d_act_hx* tanh(act_hx,if_derivative=True)\n",
    "    #d_loss/d_H\n",
    "    d_H = d_hx.T.dot(x)\n",
    "   \n",
    "    #d_loss/d_x\n",
    "    d_x = d_hx.dot(H)\n",
    "\n",
    "    #d_loss/d_d\n",
    "    d_d = d_hx.sum(axis=0,keepdims=True)\n",
    "\n",
    "\n",
    "    \n",
    "    # reshap back\n",
    "    d_x = d_x.reshape(x0.shape)\n",
    "\n",
    "\n",
    "    #     d_loss/d_C\n",
    "    d_C = np.zeros(C.shape) \n",
    "    for v in range(C.shape[0]):\n",
    "        for w in range(C.shape[1]):\n",
    "            d_C[v,w] = np.tensordot(d_X_embedding[:,:,w],X_train[:,:,v])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    C -= alpha*d_C\n",
    "    U -= alpha*d_U\n",
    "    W -= alpha*d_W\n",
    "    H -= alpha*d_H\n",
    "    d -= alpha*d_d\n",
    "    b -= alpha*d_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x123a340f0>]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training is done, let's now evaluate our NNLM to see how well it predicts text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_string,C=C,U=U,W=W,H=H,d=d,b=b,word2idx=word2idx):\n",
    "\n",
    "    # FP one time using trained weight:\n",
    "    test_list = test_string.split(\" \")\n",
    "    test_list = [[word2idx.get(i) for i in test_list]]\n",
    "    X_test = []\n",
    "    for sent in test_list:\n",
    "        X_test.append(oheY(sent,idx2word))\n",
    "    X_test = np.array(X_test)\n",
    "    x0 = X_test.dot(C) # query the word vector from Word Embedding Layer\n",
    "    x = x0.reshape(x0.shape[0],-1) # Concat: Mx(N*H)\n",
    "    hx = x.dot(H.T) + d#Mxh\n",
    "    act_hx = tanh(hx,if_derivative=False)#Mxh\n",
    "    U_act_hx = act_hx.dot(U.T) # MxV\n",
    "    Wx = x.dot(W.T) #MxV\n",
    "    \n",
    "    add_all_x = b+Wx+U_act_hx\n",
    "    output = softmax(add_all_x)\n",
    "    idxs = np.where(output==output.max())[1]\n",
    "   \n",
    "    return [idx2word.get(i) for i in idxs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icecream']"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"taste more delicious\"\n",
    "predict(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['warcraft']"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"in world of\"\n",
    "predict(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweet']"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"icecream is too\"\n",
    "predict(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for']"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"everyone is ready\"\n",
    "predict(test_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
